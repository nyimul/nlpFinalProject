# nlpFinalProject
Project name: Using Longformer with Seq2Seq and Coreference Resolution to Generate Character Analyses from Full Texts
Project team names: Ajay Bati (abati7@gatech.edu), Nyimul Hoque (nhoque3@gatech.edu), Rahul Komatineni (rkomatineni6@gatech.edu)

Abstract of project: In this paper, we present a method of approach to create summaries of charactersâ€™ stories in literary pieces, based on coreference resolution, training a Longformer model (with Seq2Seq), reinforcement learning, and character relationship graphs. We created an example dataset using coreference resolution including context with Harry Potter and the Philosopher's Stone. We extracted data from the LiSCU dataset, which is a dataset of literary pieces and their summaries paired with descriptions of characters that appear in them (presented by "Let Your Characters Tell Their Story": A Dataset for Character-Centric Narrative Understanding). We present our findings with the results from training the seq2seq hugging face model using the LongformerEncoderDecoder (LED) from Allen et. al. on the LISCU dataset and additionally evaluating on the Harry Potter coreference dataset. We also present our findings and results using reinforcement learning and generating character relationship graphs.
